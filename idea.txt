- architecture
- training


- different types of model (pre-train, fine tuned)
- 

redundant information


Todo
- xem lai torch.flatten vs GlobalAverage
- xem lai size cua position encoding
- check xem layer_norm1 voi layer_norm2 co dung chung duoc ko
- kiem tra lai xem nen dung residual truoc hay sau layernorm
- xem xem thay functional.gelu = nn.gelu duoc ko
- xem xem dung nn.Sequential trong MLP nhu transformer-zero duoc ko
- check xem dung nn.dropout trong SiglipAttention thi load duoc param cua paligemma ko
- thay nn.ModuleList bang nn.Sequential trong SiglipEncoder
- xem lai cai normalizer = torch.tensor(self.config.hidden_size**0.5, dtype=hidden_states.dtype)
- xem lai vi sao lai + voi output (nhu residual connection?) output = output * (1.0 + self.weight.float())
- xem xem trao doi vi tri qlen va self.num_heads ma ko can buoc transpose duoc ko query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1,2) # [batch_size, num_heads, q_len, head_dim]